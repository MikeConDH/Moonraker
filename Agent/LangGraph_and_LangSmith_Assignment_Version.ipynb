{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- ü§ù Breakout Room #2:\n",
        "  - Part 1:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "    3. Evaluating\n",
        "  - Part 2:\n",
        "    1. Adding conditional check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "9d599609-f2cf-4702-ed03-6598fad3cf85"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search tavily-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "9b702489-3274-4d56-f0c2-7e7c788d8f53"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "9a2f5160-bba4-4e17-867e-ef889d8ccbad"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE2 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "#from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tool_belt = [\n",
        "#    DuckDuckGoSearchRun(),\n",
        "    TavilySearchResults(), # TavilyRun() replaced duckduckgo-search because of rate limiting\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "It's a call back mechanism. The tools have a description, for example the ArxivQueryRun is for looking up technical papers and TavilyRun is for doing an Internet search. (DuckDuck had to go)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "No.  It's up to the LLM to decide when it has enough to complete.\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "You could have a retry or loop counter so it doesn't go off in an infinite loop.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Qn4n37PQRPII",
        "outputId": "1f3fb168-9f1a-4853-dbfe-dda63971dd62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"RAG in Large Language Models\"}\n",
            "Tool Response: [{'url': 'https://arxiv.org/abs/2312.10997', 'content': 'Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge ...'}, {'url': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/', 'content': 'What is Retrieval-Augmented Generation?\\nRetrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. How can AWS support your\\xa0Retrieval-Augmented Generation requirements?\\nAmazon Bedrock is a fully-managed service that offers a choice of high-performing foundation models‚Äîalong with a broad set of capabilities‚Äîto build generative AI applications while simplifying development and maintaining privacy and security. Augment the LLM prompt\\nNext, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. How does Retrieval-Augmented Generation work?\\nWithout RAG, the LLM takes the user input and creates a response based on information it was trained on‚Äîor what it already knows. What is the difference between Retrieval-Augmented Generation and semantic search?\\nSemantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications.'}, {'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'content': 'Attention mask post-processed from the retrieved documents and the question encoder input_ids by the\\nretriever.\\nquestion_encoder_last_hidden_state (tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) ‚Äî Sequence of hidden states at the output of the last layer of the question encoder pooled output of the\\nmodel.\\nquestion_enc_hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) ‚Äî Attention mask post-processed from the retrieved documents and the question encoder input_ids by the\\nretriever.\\nquestion_encoder_last_hidden_state (tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) ‚Äî Sequence of hidden states at the output of the last layer of the question encoder pooled output of the\\nmodel.\\nquestion_enc_hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) ‚Äî Attention mask post-processed from the retrieved documents and the question encoder input_ids by the\\nretriever.\\nquestion_encoder_last_hidden_state (tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) ‚Äî Sequence of hidden states at the output of the last layer of the question encoder pooled output of the\\nmodel.\\nquestion_enc_hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) ‚Äî Transformers documentation\\nRAG\\nTransformers\\nand get access to the augmented documentation experience\\nto get started\\nRAG\\nOverview\\nRetrieval-augmented generation (‚ÄúRAG‚Äù) models combine the powers of pretrained dense retrieval (DPR) and\\nsequence-to-sequence models. generator_enc_last_hidden_state (tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) ‚Äî Sequence of hidden-states at the output of the last layer of the generator encoder of the model.\\ngenerator_enc_hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) ‚Äî'}, {'url': 'https://research.ibm.com/blog/retrieval-augmented-generation-RAG', 'content': \"At IBM Research, we are focused on innovating at both ends of the process: retrieval, how to find and fetch the most relevant information possible to feed the LLM; and generation, how to best structure that information to get the richest responses from the LLM.\\nDate\\nAuthors\\nTopics\\nShare\\nThe latest AI safety method is a throwback to our maritime past\\nWhat is AI alignment?\\nCOBOL programmers are getting harder to find. Implementing RAG in an LLM-based question answering system has two main benefits: It ensures that the model has access to the most current, reliable facts, and that users have access to the model‚Äôs sources, ensuring that its claims can be checked for accuracy and ultimately trusted.\\n ‚ÄúIn a RAG system, you are asking the model to respond to a question by browsing through the content in a book, as opposed to trying to remember facts from memory.‚Äù\\n What is retrieval-augmented generation?\\nRAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.\\n In the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant.\"}, {'url': 'https://arxiv.org/abs/2405.06211', 'content': \"Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, retrieval-augmented large language models have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the generation quality of LLMs.\"}]\n",
            "\n",
            "Agent Response: Retrieval-Augmented Generation (RAG) is a process in Large Language Models (LLMs) that optimizes the output by referencing an authoritative knowledge base outside of its training data sources before generating a response. RAG has emerged as a promising solution to challenges faced by LLMs, such as hallucination, outdated knowledge, and non-transparent reasoning processes. It enhances the accuracy and credibility of the generation, particularly for knowledge retrieval.\n",
            "\n",
            "RAG broke onto the scene as a solution to improve the generation quality of LLMs by incorporating external and authoritative knowledge bases. It ensures that the model has access to the most current and reliable facts, and users have insight into the generative process of LLMs. RAG allows LLMs to ground themselves on accurate and up-to-date information from external sources, leading to more trustworthy and informative responses.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "#inputs = {\"messages\" : [HumanMessage(content=\"What is RAG?\")]}\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <<< DUCK DUCK GO RATELIMIT EXCEPTION >>> \n",
        "Attempting to ask about QLoRA can cause an error.\n",
        "MOVED TO TAVILY "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "afv2BuEsV5JG",
        "outputId": "4d732ee1-5645-4c01-b236-7eec7f806753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA in Machine Learning\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2023-12-31\n",
            "Title: Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\n",
            "Authors: Dipankar Sarkar\n",
            "Summary: This paper aims to introduce and analyze the Viz system in a comprehensive\n",
            "way, a novel system architecture that integrates Quantized Low-Rank Adapters\n",
            "(QLoRA) to fine-tune large language models (LLM) within a legally compliant and\n",
            "resource efficient marketplace. Viz represents a significant contribution to\n",
            "the field of artificial intelligence, particularly in addressing the challenges\n",
            "of computational efficiency, legal compliance, and economic sustainability in\n",
            "the utilization and monetization of LLMs. The paper delineates the scholarly\n",
            "discourse and developments that have informed the creation of Viz, focusing\n",
            "primarily on the advancements in LLM models, copyright issues in AI training\n",
            "(NYT case, 2023), and the evolution of model fine-tuning techniques,\n",
            "particularly low-rank adapters and quantized low-rank adapters, to create a\n",
            "sustainable and economically compliant framework for LLM utilization. The\n",
            "economic model it proposes benefits content creators, AI developers, and\n",
            "end-users, delineating a harmonious integration of technology, economy, and\n",
            "law, offering a comprehensive solution to the complex challenges of today's AI\n",
            "landscape.\n",
            "\n",
            "Published: 2024-02-08\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technolog\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"Tim Dettmers bio\"}\n",
            "Tool Response: [{'url': 'https://timdettmers.com/about/', 'content': 'Tim Dettmers, Luke Zettlemoyer. [bib] 2018. Convolutional 2D Knowledge Graph Embeddings, Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel. AAAI2018. 2016. 8-Bit Approximations for Parallelism in Deep Learning, Tim Dettmers. ICLR2016.'}, {'url': 'https://scholar.google.com/citations?user=lHI3w5kAAAAJ', 'content': 'Tim Dettmers. University of Washington. Verified email at cs.washington.edu - Homepage. Deep Learning Natural Language Processing. Articles Cited by Public access Co-authors. Title. ... A Borzunov, M Ryabinin, T Dettmers, Q Lhoest, L Saulnier, M Diskin, ... NeurIPS 2021 Demonstration, 2022. 9: 2022:'}, {'url': 'https://huggingface.co/timdettmers', 'content': '261 followers. ¬∑. 1 following. https://timdettmers.com. tim_dettmers. timdettmers. None yet. Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA. A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes.'}, {'url': 'https://tech.cornell.edu/events/seminar-cornell-tech-tim-dettmers/', 'content': \"Speaker Bio. Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning.\"}, {'url': 'https://timdettmers.com/', 'content': 'Deep Learning Hardware Limbo. 2017-12-21 by Tim Dettmers 91 Comments. With the release of the Titan V, we now entered deep learning hardware limbo. It is unclear if NVIDIA will be able to keep its spot as the main deep learning hardware vendor in 2018 and both AMD and Intel Nervana will have a shot at overtaking NVIDIA.'}]\n",
            "\n",
            "Agent Response: The QLoRA paper in Machine Learning is titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. The paper introduces QLoRA, an efficient finetuning approach that reduces memory usage to finetune large language models on a single GPU while preserving performance. It introduces innovations like 4-bit NormalFloat (NF4) data type and double quantization to save memory without sacrificing performance.\n",
            "\n",
            "The first author of the QLoRA paper is Tim Dettmers. Tim Dettmers is a researcher focused on making foundation models like ChatGPT accessible to researchers and practitioners by reducing their resource requirements. He works on developing compression and networking algorithms and building systems for memory-efficient, fast, and cost-effective deep learning.\n",
            "\n",
            "You can find more information about Tim Dettmers on his [website](https://timdettmers.com/about/).\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "Steps 1 - Entry Point.  The User submits the question as a message: \"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
        "\n",
        "Step 2 - LangChain Involke.  This is where all the action takes place, it kicks off the agentic RAG flow to process the message.  The question is passed to the entry point\n",
        "\n",
        "Step 3 - Agent has two tools tavily and arxiv, querry QLora in Manchine learning,  Action is arxiv getting info on QLora. Continue.\n",
        "\n",
        "Step 4 - Agent has two tools tavily and arxiv, querry Tim Dettmers bio. Action is tavily getting info from timdetters.com. Continue.\n",
        "\n",
        "Step 5 - LLMs has what it needs, composes output and says End. Responsd to the user.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <<< TRIVAY LIMITATION >>>\n",
        "After experimenting (creating a langgraph with only trivaly, and reviewing the langsmith logs),\n",
        "I noticed that you will get a 400 error on occation: HTTPError('400 Client Error: Bad Request for url: https://api.tavily.com/search') \n",
        "\n",
        "When put into an agent chain, it does not throw an error, but politely informs of the issue.  If you increase the specificity of the querry (What is RAG for LLM Applications), it works - see below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I encountered an error while trying to retrieve information about RAG. Would you like me to try again or search for information using a different source?'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "6eda06b2-0110-44c0-8106-b1280376a2c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for \"Retrieval-Augmented Generation,\" which is a model architecture that combines retrieval-based and generation-based approaches in natural language processing tasks. In the context of LLM (Large Language Models) applications, RAG can be used to improve the performance of language models by incorporating information retrieval techniques to enhance the generation of text.\\n\\nThe RAG model typically consists of two main components:\\n\\n1. Retrieval Component: This component is responsible for retrieving relevant information from a large corpus of text or knowledge base. It uses retrieval algorithms to find the most relevant passages or documents that can help generate accurate and contextually relevant responses.\\n\\n2. Generation Component: This component is responsible for generating text based on the retrieved information. It uses the retrieved passages as context to generate coherent and informative responses to queries or prompts.\\n\\nBy combining these two components, the RAG model can leverage the strengths of both retrieval and generation approaches to improve the quality and relevance of generated text in LLM applications.\\n\\nIf you would like more detailed information or examples of RAG in LLM applications, please let me know.'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG for LLM Applications\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "## Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "\n",
        "The correct answers must be an exact match to be considered.  This does not accomodate for any semantic match, like near matches that should be considered.  For example, If the match word is \"Collabates\" and your resume contains \"Collaboration\", the ATS will reject you.  So, yeah, in some cases, it may be problematic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "## Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "\n",
        "Answer:\n",
        "Langchain provides other metrics like correctness, releance, fluency, conciseness, coverage and sentiment.\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "045f1296-2bee-43c8-8b92-b24dc242ff88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 919beb19' at:\n",
            "https://smith.langchain.com/o/4a61d98d-132a-5d07-84b0-e576bf509efa/datasets/42c0ab01-71bf-4d1b-a010-59d38ce9b2f7/compare?selectedSessions=3581f10f-589f-49a8-a1e5-f3dc4bf85dc9\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 695475b5 at:\n",
            "https://smith.langchain.com/o/4a61d98d-132a-5d07-84b0-e576bf509efa/datasets/42c0ab01-71bf-4d1b-a010-59d38ce9b2f7\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.COT Contextual Accuracy</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>c8026345-ac0a-4c86-9c10-1841ecdbeb04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.255837</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.447214</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.435169</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.740964</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.420380</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.620947</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.115952</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.917685</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.COT Contextual Accuracy  \\\n",
              "count               6.000000                          5.000000   \n",
              "unique                   NaN                               NaN   \n",
              "top                      NaN                               NaN   \n",
              "freq                     NaN                               NaN   \n",
              "mean                0.833333                          0.800000   \n",
              "std                 0.408248                          0.447214   \n",
              "min                 0.000000                          0.000000   \n",
              "25%                 1.000000                          1.000000   \n",
              "50%                 1.000000                          1.000000   \n",
              "75%                 1.000000                          1.000000   \n",
              "max                 1.000000                          1.000000   \n",
              "\n",
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      6     0        6.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                     True   NaN             NaN   \n",
              "freq                       3   NaN             NaN   \n",
              "mean                     NaN   NaN        4.255837   \n",
              "std                      NaN   NaN        2.435169   \n",
              "min                      NaN   NaN        1.740964   \n",
              "25%                      NaN   NaN        3.420380   \n",
              "50%                      NaN   NaN        3.620947   \n",
              "75%                      NaN   NaN        4.115952   \n",
              "max                      NaN   NaN        8.917685   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     c8026345-ac0a-4c86-9c10-1841ecdbeb04  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 919beb19',\n",
              " 'results': {'8d6a6f18-79f4-40ea-b73b-c59dc4d6457b': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question asked, which is about the optimizer used in QLoRA. The answer includes the name of the optimizer and also provides additional information on how to access it. This information is relevant and could be useful to someone who is working with QLoRA and needs to know about its optimizer. \\n\\nTherefore, the submission is helpful, insightful, and appropriate, meeting the given criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3743d88b-33f1-4546-99e9-883bf920c436'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=None, value=None, comment=\"The context does not provide enough information to accurately grade the student's answer. The context only mentions 'paged' and 'optimizer', but does not specify what optimizer is used in QLoRA. Therefore, we cannot confirm or deny the student's answer based on the provided context.\\nGRADE: Cannot be determined.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('603bbe91-14a2-44b8-b61c-2cac50aa899a'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('b2f317b1-8cf8-4cb0-968d-6860dda03aa0'), target_run_id=None)],\n",
              "   'execution_time': 3.678266,\n",
              "   'run_id': 'c8026345-ac0a-4c86-9c10-1841ecdbeb04',\n",
              "   'output': 'The optimizer used in QLoRA is the Paged Optimizer, which can be accessed with the argument \"--optim paged_adamw_32bit\".',\n",
              "   'reference': {'must_mention': ['paged', 'optimizer']}},\n",
              "  '4b7f9aa5-af0e-4eeb-8303-cced65613e44': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a clear and direct answer to the question, stating that the QLoRA paper introduced a new data type called \"4-bit NormalFloat (NF4)\". This directly answers the question, making it helpful.\\n\\nThe submission also provides additional information about the purpose and use of the new data type, stating that it is \"information theoretically optimal for normally distributed weights\" and was designed to \"save memory without sacrificing performance in the context of finetuning language models\". This additional information provides context and insight, making the submission even more helpful.\\n\\nThe submission is also appropriate, as it directly addresses the question and provides relevant information.\\n\\nTherefore, the submission meets the criterion of being helpful.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e4c71843-49fe-43c6-bf56-8ecdeb87447e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context mentions 'NF4' and 'NormalFloat' as the data type created in the QLoRA paper. The student's answer also mentions '4-bit NormalFloat (NF4)' as the data type introduced in the QLoRA paper. The student's answer aligns with the context provided. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('579e22b9-e759-42c5-83a2-81f1b133f6c1'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('70f922f7-cd1b-417e-98b2-f19eebed6e1a'), target_run_id=None)],\n",
              "   'execution_time': 3.37263,\n",
              "   'run_id': '55dbc353-8b74-41e2-a6c3-0be109256e21',\n",
              "   'output': 'The QLoRA paper introduced a new data type called \"4-bit NormalFloat (NF4)\" which is information theoretically optimal for normally distributed weights. This data type was designed to save memory without sacrificing performance in the context of finetuning language models.',\n",
              "   'reference': {'must_mention': ['NF4', 'NormalFloat']}},\n",
              "  '03823f30-a85c-4c7e-8934-85d7a05a36bb': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. It explains the components of a RAG system, how it works, and where it is commonly used. \\n\\nThe submission is insightful as it provides information about the benefits of using a RAG system and how it improves the performance of text generation models. \\n\\nThe submission is appropriate as it directly answers the question asked in the input and provides relevant information about the topic. \\n\\nBased on these observations, the submission meets the criterion of being helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d9e52168-7519-4c24-92bf-97e1e201186b'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer provides a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. The student correctly identifies that a RAG system is a type of natural language processing system that combines elements of information retrieval and text generation. The student also correctly explains the role of the retrieval component in a RAG system and the benefits of using such a system. The student also correctly identifies some of the tasks where RAG systems are commonly used. The student's answer does not conflict with the context provided.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d9677d5b-78ae-47c5-a2ae-891e79de16e3'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('8b31edea-bf13-4ab6-8d8e-e1229134750b'), target_run_id=None)],\n",
              "   'execution_time': 4.261847,\n",
              "   'run_id': 'de57de03-2ae2-4997-80a5-707511ade06b',\n",
              "   'output': 'A Retrieval Augmented Generation (RAG) system is a type of natural language processing system that combines elements of information retrieval and text generation. In a RAG system, a retrieval component is used to search for relevant information or context from a large corpus of text, and this retrieved information is then used to generate a response or output.\\n\\nThe retrieval component in a RAG system helps to provide the model with relevant information that can be used to enhance the quality and relevance of the generated text. This approach allows the system to generate more accurate and contextually relevant responses by leveraging information from external sources.\\n\\nRAG systems are commonly used in tasks such as question answering, dialogue generation, and content creation, where access to external knowledge or context is important for generating high-quality outputs. These systems have been shown to improve the performance of text generation models by incorporating external information during the generation process.',\n",
              "   'reference': {'must_mention': ['ground', 'context']}},\n",
              "  '2e00ba56-5460-4211-b432-9f3e644fccbc': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission is supposed to be helpful, insightful, and appropriate. \\n\\nLooking at the input, the question asked is about the authorship of the QLoRA paper. \\n\\nThe submission provided the authors of the QDyLoRA paper, not the QLoRA paper. This is a mismatch between the question and the answer provided. \\n\\nTherefore, the submission is not helpful or appropriate as it does not answer the question asked. \\n\\nThe submission does not meet the criteria. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2f43367f-aa03-4336-ba42-40f2f33ff6b0'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=\"The student's answer does not match the context provided. The context states that the authors of the QLoRA paper are 'Tim' and 'Dettmers', but the student has listed a completely different set of authors. Therefore, the student's answer is incorrect.\\nGRADE: INCORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('93de2386-312b-473b-9af7-c93edf2d109a'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('bf52697d-e4e6-43f4-a17c-b9788fe49ac7'), target_run_id=None)],\n",
              "   'execution_time': 3.563629,\n",
              "   'run_id': '621572f8-cb1b-449a-8064-32c6ab630fc8',\n",
              "   'output': 'The QDyLoRA paper was authored by Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, and Mehdi Rezagholizadeh.',\n",
              "   'reference': {'must_mention': ['Tim', 'Dettmers']}},\n",
              "  'faeed01a-0e20-4694-827e-34c6274333fa': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a clear answer to the question, stating that TensorFlow is the most popular deep learning framework. \\n\\nThe submission goes beyond just providing a name, it also gives a brief description of TensorFlow, explaining that it is an open-source machine learning library developed by Google. This additional information is helpful for someone who may not be familiar with TensorFlow.\\n\\nThe submission also mentions other popular deep learning frameworks, which provides additional helpful information for the reader.\\n\\nBased on these points, the submission is helpful, insightful, and appropriate. \\n\\nTherefore, the submission meets the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('360b30a3-0936-4cd3-aa93-964d370e8a82'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer states that TensorFlow is the most popular deep learning framework. This aligns with the context provided, which lists TensorFlow as one of the popular deep learning frameworks. The student also provides additional information about TensorFlow, which does not contradict the context. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4361fe72-0aab-43a0-890c-012dc9bc580d'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('5f4116c4-96ad-4149-a46e-c737aa798733'), target_run_id=None)],\n",
              "   'execution_time': 1.740964,\n",
              "   'run_id': '92073979-527e-4940-b60c-9e91f899cc7e',\n",
              "   'output': 'The most popular deep learning framework is TensorFlow. TensorFlow is an open-source machine learning library developed by Google that is widely used for building and training deep learning models. It provides a comprehensive ecosystem of tools, libraries, and community support for deep learning research and applications. Other popular deep learning frameworks include PyTorch, Keras, and Caffe.',\n",
              "   'reference': {'must_mention': ['PyTorch', 'TensorFlow']}},\n",
              "  '0f373ffd-4d97-4230-87cf-ed06d2b6ffbc': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it provides a detailed explanation of the LoRa system and the significant improvements it offers. The information is presented in a clear and organized manner, making it easy for the reader to understand.\\n\\nThe submission is helpful as it provides valuable information about the LoRa system. It explains what the LoRa system is and lists six significant improvements it offers, including long-range communication, low power consumption, scalability, cost-effectiveness, security, and flexibility. Each point is explained in detail, providing the reader with a comprehensive understanding of the topic.\\n\\nThe submission is insightful as it not only lists the improvements but also explains why these improvements are significant. For example, it explains that the long-range communication capability of the LoRa system is ideal for applications that require connectivity over large geographic areas. This insight helps the reader understand the practical implications of the improvements.\\n\\nThe submission is appropriate as it directly answers the given input. The input asks for the significant improvements the LoRa system makes, and the submission provides a detailed answer to this question.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3741cc4e-584b-46a5-931d-d9143f808120'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer provides a detailed explanation of the improvements that the LoRa system offers. However, the context provided for grading the answer is ['reduce', 'parameters']. This context is vague and does not provide specific parameters that the LoRa system is supposed to improve. Therefore, it is difficult to accurately grade the student's answer based on this context. However, based on general knowledge of the LoRa system, the student's answer appears to be factually accurate, as it correctly identifies several improvements that the LoRa system offers, such as long-range communication, low power consumption, scalability, cost-effectiveness, security, and flexibility. \\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('950413c2-a6ba-4d77-8344-a28acb6cd725'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6d037e33-e2db-4d93-a948-5d7a33d4be2b'), target_run_id=None)],\n",
              "   'execution_time': 8.917685,\n",
              "   'run_id': '90f7b56e-5c4b-4825-b980-ca1b926b415a',\n",
              "   'output': \"The LoRa system, which stands for Long Range, is a low-power wide-area network (LPWAN) technology that enables long-range communication for Internet of Things (IoT) devices. Some significant improvements that the LoRa system offers include:\\n\\n1. Long Range Communication: LoRa technology enables communication over long distances, up to several kilometers in urban areas and even tens of kilometers in rural areas. This long-range capability is ideal for applications that require connectivity over large geographic areas.\\n\\n2. Low Power Consumption: LoRa devices are designed to operate on low power, making them suitable for battery-operated IoT devices. The low power consumption of LoRa devices allows for long battery life, reducing the need for frequent battery replacements or recharging.\\n\\n3. Scalability: The LoRa system is highly scalable, allowing for the deployment of a large number of devices within a single network. This scalability makes it suitable for applications that require connectivity for a large number of IoT devices spread across a wide area.\\n\\n4. Cost-Effective: LoRa technology is cost-effective compared to other wireless communication technologies, making it an attractive option for IoT deployments that require long-range connectivity at a lower cost.\\n\\n5. Secure Communication: The LoRa system provides secure communication through encryption and authentication mechanisms, ensuring that data transmitted between devices is protected from unauthorized access or tampering.\\n\\n6. Flexibility: LoRa technology offers flexibility in terms of deployment options, allowing for both public and private network configurations. This flexibility enables organizations to choose the network setup that best suits their specific requirements.\\n\\nOverall, the LoRa system's significant improvements in long-range communication, low power consumption, scalability, cost-effectiveness, security, and flexibility make it a popular choice for IoT applications that require reliable and efficient connectivity over large distances.\",\n",
              "   'reference': {'must_mention': ['reduce', 'parameters']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "## Part 2:\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8t-4FISCEh"
      },
      "source": [
        "We're going to add a custom helpfulness check here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!\n",
        "\n",
        "check_helpfulness() evaluates the response to a query.  Based on the state, it gets the intial query and final response, it checks that number of state messages are under 10, and if not it returns end.  It creates the prompt template, constructing a chain of the prompt, GPT-4 as the model, and passing to a string output parser.  It involves the chain and if it has Y, then it ends, and if not continues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OWOPweuSiHc"
      },
      "source": [
        "Let's compile and test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "7e2ea696-ba42-468e-a842-2c585f246c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\n",
            "\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"LoRA in machine learning\"}\n",
            "Tool Response: [{'url': 'https://arxiv.org/abs/2106.09685', 'content': 'An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is ...'}, {'url': 'https://www.datacamp.com/tutorial/mastering-low-rank-adaptation-lora-enhancing-large-language-models-for-efficient-adaptation', 'content': 'Natassha Selvaraj\\n8 min\\nWhat Fortune 1000 Executives Believe about Data & AI in 2024 with Randy Bean, Innovation Fellow, Data Strategy, Wavestone\\nRichie Cotton\\n46 min\\nData Security in the Age of AI with Bart Vandekerckhove, Co-founder at Raito\\nRichie Cotton\\n46 min\\nBuilding Trustworthy AI with Alexandra Ebert, Chief Trust Officer at MOSTLY AI\\nRichie Cotton\\n50 min\\nHow Transformers Work: A Detailed Exploration of Transformer Architecture\\nJosep Ferrer\\n15 min\\nCross-Entropy Loss Function in Machine Learning: Latest news about our products and team\\nDiscover content by tools and technology\\nDiscover content by data science topics\\nMastering Low-Rank Adaptation (LoRA): Enhancing Large Language Models for Efficient Adaptation\\nLoRA: Low-Rank Adaptation of Large Language Models\\nThe field of machine learning and natural language processing (NLP) has witnessed a remarkable advancement with the introduction of Large Language Models (LLMs) such as GPT, LLaMa, Claude 2, etc. Benefits of pre-fix tuning:\\nLoRA and prefix tuning can be combined within the PEFT (Parameter Efficient Fine-Tuning) framework:\\nExample of LoRA implementation using Loralib library in Python\\nTo implement LoRA, you can use the Loralib library by Microsoft. What Fortune 1000 Executives Believe about Data & AI in 2024 with Randy Bean, Innovation Fellow, Data Strategy, Wavestone\\nData Security in the Age of AI with Bart Vandekerckhove, Co-founder at Raito\\nBuilding Trustworthy AI with Alexandra Ebert, Chief Trust Officer at MOSTLY AI\\nHow Transformers Work: A Detailed Exploration of Transformer Architecture\\nCross-Entropy Loss Function in Machine Learning: Advantages of LoRA\\nThere are several advantages that come with using LoRA for fine-tuning:\\n1. Efficiency in training and deployment\\nLoRA reduces the computational burden, allowing faster adaptation of models.'}, {'url': 'https://towardsdatascience.com/lora-intuitively-and-exhaustively-explained-e944a6bff46b', 'content': 'More from Daniel Warfield and Towards Data Science\\nDaniel Warfield\\nin\\nTowards Data Science\\nTransformers\\u200a‚Äî\\u200aIntuitively and Exhaustively Explained\\nExploring the modern wave of machine learning: taking apart the transformer step by step\\n--\\n14\\nRahul Nayak\\nin\\nTowards Data Science\\nHow to Convert Any Text Into a Graph of Concepts\\nA method to convert any text corpus into a Knowledge Graph using Mistral 7B.\\n--\\n35\\nMarco Peixeiro\\nin\\nTowards Data Science\\nTimeGPT: The First Foundation Model for Time Series Forecasting\\nExplore the first generative pre-trained forecasting model and apply it in a project with Python\\n--\\n22\\nDaniel Warfield\\nin\\nTowards Data Science\\nConvolutional Networks\\u200a‚Äî\\u200aIntuitively and Exhaustively Explained\\nUnpacking a cornerstone modeling strategy\\n--\\n3\\nRecommended from Medium\\nRahul Nayak\\nin\\nTowards Data Science\\nHow to Convert Any Text Into a Graph of Concepts\\nA method to convert any text corpus into a Knowledge Graph using Mistral 7B.\\n--\\n35\\nAlden Do Rosario\\nLangchain is NOT for production use. From MLOps to mounting‚Ä¶\\n--\\n20\\nLists\\nPredictive Modeling w/ Python\\nPractical Guides to Machine Learning\\nNatural Language Processing\\nNew_Reading_List\\nPaul Rose\\nI Found A Very Profitable AI Side Hustle\\nAnd it‚Äôs perfect for beginners\\n--\\n192\\nDevansh\\nin\\nDataDrivenInvestor\\nWhy Elon Musks AI Model Grok is the future of LLMs\\nWhy Big Tech Companies should copy the Grok approach ASAP\\n--\\n11\\nThe Pareto Investor\\nNASA Just Shut Down Quantum Computer After Something Insane Happened!\\n Houston, We Have a Problem!\\n--\\n136\\nGathnex\\nMistral-7B Fine-Tuning: A Step-by-Step Guide\\nIntroducing Mistral 7B: The Powerhouse of Language Models\\n--\\n6\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams Member-only story\\nNatural Language Processing | Machine Learning\\nLoRA ‚Äî Intuitively and Exhaustively Explained\\nExploring the modern wave of machine learning with cutting edge fine tuning\\nDaniel Warfield\\nFollow\\nTowards Data Science\\n--\\n7\\nShare\\nFine tuning is the process of tailoring a machine learning model to a specific application, which can be vital in achieving consistent and high quality performance. What, and Why, is Fine Tuning?\\n--\\n--\\n7\\nWritten by Daniel Warfield\\nTowards Data Science\\nData Scientist, Educator, Artist, Writer.\\n'}, {'url': 'https://machinelearningmastery.com/using-lora-in-stable-diffusion/', 'content': 'LoRA, or Low-Rank Adaptation, is a lightweight training technique used for fine-tuning Large Language and Stable Diffusion Models without needing full model training. Full fine-tuning of larger models (consisting of billions of parameters) is inherently expensive and time-consuming. LoRA works by adding a smaller number of new weights to the ...'}, {'url': 'https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578', 'content': 'DBS Bank | Youtube: https://www.youtube.com/channel/UCQoNosQTIxiMTL9C-gvFdjA\\nMore from Mehul Gupta and Data Science in your pocket\\nMehul Gupta\\nin\\nData Science in your pocket\\nRecommendation Systems using Langchain and LLMs with codes\\nusing RAG framework and chains\\n--\\n3\\nMehul Gupta\\nin\\nData Science in your pocket\\nLangchain tutorials for newbies\\nLangchain use cases with demo explained\\n--\\n2\\nMehul Gupta\\nin\\nData Science in your pocket\\nWhat are Vector Databases and How Langchain uses Vector DBs\\nwith codes and examples\\n--\\n1\\nMehul Gupta\\nin\\nData Science in your pocket\\nBest AI-Agents you should know\\nHuggingGPT, DemoGPT, AutoGPT and more\\n--\\nRecommended from Medium\\nBijit Ghosh\\nAdvanced Techniques for Fine-Tuning LLMs\\nIntroduction\\n--\\nGaurav Garg\\nin\\nGoPenAI\\nUnderstanding LLM Fine Tuning\\u200a‚Äî\\u200aA Complete Guide for Everyone\\nThe Rise of Large Language Models and Fine Tuning\\n--\\nLists\\nPredictive Modeling w/ Python\\nChatGPT prompts\\nChatGPT\\nAI Regulation\\nDhanoop Karunakaran\\nin\\nIntro to Artificial Intelligence\\nFine-tuning Large Language Models series: Part1\\u200a‚Äî\\u200aInternal mechanism of LLMs\\n Akriti Upadhyay\\nEmploying QdrantDB to conduct advanced similarity searches for image data\\nImplementing Image Similarity Search using Qdrant\\n--\\n1\\nAbhinav Kimothi\\nin\\nMLearning.ai\\nWhat is a fine-tuned LLM?\\nFine-tuning large language models (LLMs) has become a powerful technique for achieving impressive performance in various natural language‚Ä¶\\n--\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams Have a look below\\nWe will start off with pip installing libraries\\nNext, import the important functions required\\nNext, let‚Äôs load the training and test dataset alongside the LLM to be fine-tuned with its tokenizer\\nNext, using the Input and Output, we will create a prompt template which is a requirement by the SFTTrainer we will be using later\\nNow is the time we set the trainer for LoRA\\n LoRA for Fine-Tuning LLMs explained with codes and example\\nHow to fine-tune your LLMs faster using LoRA\\nMehul Gupta\\nFollow\\nData Science in your pocket\\n--\\nListen\\nShare\\nThis entire year in AI space has been revolutionary because of the advancements in Gen-AI especially the incoming of LLMs. Once done, login into huggingface-hub using the WRITE token in the Jupyter Notebook and push your model using the below code\\nNow, upload the model\\nNow, once uploaded, again use the notebook login but using the READ token this time\\nNow run the below code for inferencing with your private model\\nSee the output for yourself\\nWith this, we will be wrapping up this very long post.'}]\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"Tim Dettmers\"}\n",
            "Tool Response: [{'url': 'https://timdettmers.com/about/', 'content': 'Tim Dettmers, Luke Zettlemoyer. [bib] 2018. Convolutional 2D Knowledge Graph Embeddings, Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel. AAAI2018. 2016. 8-Bit Approximations for Parallelism in Deep Learning, Tim Dettmers. ICLR2016.'}, {'url': 'https://scholar.google.com/citations?user=lHI3w5kAAAAJ', 'content': 'Tim Dettmers. University of Washington. Verified email at cs.washington.edu - Homepage. Deep Learning Natural Language Processing. Articles Cited by Public access Co-authors. Title. ... A Borzunov, M Ryabinin, T Dettmers, Q Lhoest, L Saulnier, M Diskin, ... NeurIPS 2021 Demonstration, 2022. 9: 2022:'}, {'url': 'https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/', 'content': 'Now I have a simple question: should I use the integrated graphics on CPU to connect with the monitor for display purposes?\\nDoes connecting the graphics card with a dual-monitor in QHD affect the performance of the graphics card during training?\\nThanks, Shuhao\\nTim Dettmers says\\n2020-11-06 at 15:34\\nUsually, the displays do not need that much memory. I want to thank Agrin Hilmkil, Ari Holtzman, Gabriel Ilharco, Nam Pho for their excellent feedback on the previous version of this blog post.\\nRelated\\nRelated Posts\\nFiled Under: Deep Learning, Hardware Tagged With: AMD, CPU, High Performance Computing, Matrix Multiplication, Parallel Computing, PCIe Lanes, Sparse Training\\nReader Interactions\\nComments\\nZoran says\\n2023-04-30 at 08:21\\nHello,\\nHave you had any chance to test high end CPU only interface, for example on Intel 13900K CPU? I am looking to buy a GPU to use for deep learning (Computer vision, objection detection & NLP using neural networks ‚Äì RCNN, FCN, Yolo , SSD, CTPN, EAST‚Ä¶etc) I am a bit tight in budget, I was looking at the 1060 (6GB) as a minimum, how does it compare to the 1660 for the same purpose?\\nread in one of your comments above that 1660 does not ave tensor cores and maybe good for gaming but not for deep learning‚Ä¶ which one do you suggest? For past updates of this blog post, I want to thank Mat Kelcey for helping me to debug and test custom code for the GTX 970; I want to thank Sander Dieleman for making me aware of the shortcomings of my GPU memory advice for convolutional nets; I want to thank Hannes Bretschneider for pointing out software dependency problems for the GTX 580; and I want to thank Oliver Griesel for pointing out notebook solutions for AWS instances. In your Quora article, you wrote:\\n‚ÄúHowever, if you now use a fleet of either Ferraris and big trucks (thread parallelism), and you have a big job with many packages (large chunks of memory such as matrices) then you will wait for the first truck a bit, but after that you will have no waiting time at all ‚Äî unloading the packages takes so much time that all the trucks will queue in unloading location B so that you always have direct access to your packages (memory).'}, {'url': 'https://timdettmers.com/', 'content': 'Tim Dettmers is a researcher and educator in deep learning and machine learning. His blog covers topics such as GPU choice, grad school selection, creativity, sparse networks, and TPUs vs GPUs for transformers.'}, {'url': 'https://github.com/TimDettmers', 'content': 'You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window.'}]\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"Attention in machine learning\"}\n",
            "Tool Response: [{'url': 'https://towardsdatascience.com/introduction-to-attention-mechanism-8d044442a29', 'content': \"That‚Äôs why you‚Äôre not using just a dot product, but a scaled dot product, that way our new formula looks like\\nIf you‚Äôre a having problem understanding why dot product creates a large number with high dimensional vectors please check 3Blue1Brown‚Äôs Youtube video on the subject\\nAdditionally, we want to be able to use more than one query vector q. It was great to have a single query vector for each timestamp of the decoder but it can be a lot simpler when we use all of them at the same time, so we change our vector to vectors Q (Shape NQ\\u200b√óDQ\\u200b). References:\\nOriginally published at https://erdem.pl.\\n--\\n--\\nWritten by Kemal Erdem (burnpiro)\\nTowards Data Science\\nML Engineer, Javascript Architect, Consultant, MTB lover\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams The reason was to match the colors used in The Illustrated Transformer blog post.\\nConclusions\\nFrom this point onwards we can use the Self-Attention Layer to create a Transformer but this article is too long already. In the case of the token woman, both people on the image save similar attention weights but that's still ok because the model could decide which one is the subject and how to name that person.\\n Now we have to pass the same grid and s0\\u200b to the alignment function to calculate the corresponding alignment score for each value of the grid\\nThat gives us alignment scores for t=1 timestep.\"}, {'url': 'https://towardsdatascience.com/attaining-attention-in-deep-learning-a712f93bdb1e', 'content': 'It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way ‚Äî in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\\n Whether it is a translation or a QnA task, in which the input is a question and a paragraph and the model needs to predict the answer to that question based on the paragraph or, any other sequence to sequence modeling operation, the intermediary state continues to be the most crucial piece of the puzzle.\\n Once, we have associated energies corresponding to all annotations, we do a softmax to obtain the global alignment weights ‚ç∫‚Çñ ‚±º.\\nStep 2, mentioned before, can now be broken down as follows:\\nTraining/Loss Calculation\\nThe alignment model directly computes a soft alignment ‚Äî considers all inputs, which allows the gradient of the loss function ‚Äî calculated for final outputs of the entire sequence to sequence model ‚Äî to be back-propagated through. This car is 2m in height ‚Äî so lean on my friend, 12m in width ‚Äî not very hug-able unfortunately, with a 8m wheelbase ‚Äî whatever use that information is for, a turning radius of just 0.1m ‚Äî making it easier to turn away from all your problems, a boot-space of 200 litres ‚Äî for all the luggage you‚Äôll carry for a trip and never use, a ground clearance of 0.25m ‚Äî in case you ever seek refuge under the car, a 6 cylinder engine with 5 valves ‚Äî a spec you‚Äôll only ever use while showing off and lastly has a dual overhead camshaft because big cars comes with big words. The Inputs\\nWe input both decoder hidden state and the annotations in our neural network to predict a single value ‚Äî e‚Çñ ‚±º as the authors of the paper liked to call it, the ‚Äúassociated energy‚Äù‚Äî signifying the importance of the annotations in next decoder step H‚Çñ.'}, {'url': 'https://en.wikipedia.org/wiki/Attention_(machine_learning)', 'content': 'Note that the context vector for \"that\" does not rely on context vectors for the other words; therefore the context vectors of all words can be calculated using the whole matrix X, which includes all the word embeddings, instead of a single word\\'s embedding vector x in the formula above, thus parallelizing the calculations. More correctly, we should take the transpose of the context vector and use the column-wise softmax, resulting in the more correct form\\nThe query vector is compared (via dot product) with each word in the keys. The structure of the input data is captured in the Qw and Kw weights, and the Vw weights express that structure in terms of more meaningful features for the task being trained for.\\n On the first pass through the decoder, 94% of the attention weight is on the first English word \"I\", so the network offers the word \"je\".\\n Multiplying this against the value matrix effectively amplifies the signal for the most important words in the sentence and diminishes the signal for less important words.[9]\\n'}, {'url': 'https://machinelearningmastery.com/what-is-attention/', 'content': 'The human brain does so by relying on attention, such that it dynamically stores in memory the information that the human subject most pays attention to.\\nAttention in Machine Learning\\nImplementing the attention mechanism in artificial neural networks does not necessarily track the biological and psychological mechanisms of the human brain. A list of these vectors (the second component of the attention-based system above), together with the decoder‚Äôs previous hidden states, will be exploited by the attention mechanism to dynamically highlight which of the input information will be used to generate the output.\\n LinkedIn |\\nTwitter |\\nFacebook |\\nNewsletter |\\nRSS\\nPrivacy |\\nDisclaimer |\\nTerms |\\nContact |\\nSitemap |\\nSearch Instead, it is the ability to dynamically highlight and use the salient parts of the information at hand‚Äîin a similar manner as it does in the human brain‚Äîthat makes attention such an attractive concept in machine learning.\\n At each time step, the attention mechanism then takes the previous hidden state of the decoder and the list of encoded vectors, using them to generate unnormalized score values that indicate how well the elements of the input sequence align with the current output.'}, {'url': 'https://www.geeksforgeeks.org/ml-attention-mechanism/', 'content': 'We can observe 3 sub-parts or components of the Attention Mechanism architecture :\\nConsider the following Encoder-Decoder architecture with Attention.\\nEncoder-Decoder with Attention\\nEncoder:\\nThe encoder applies recurrent neural networks (RNNs) or transformer-based models to iteratively process the input sequence. Data Structures and Algorithms\\nML & Data Science\\nWeb Development\\nLanguages\\nInterview Corner\\nCS Subjects\\nJobs\\nPractice\\nContests\\nML ‚Äì Attention mechanism\\nLet‚Äôs take a look at hearing and a case study of selective attention in the context of a crowded cocktail party. The context vector is fed into the decoder along with the current hidden state of the decoder in order to predict the next token in the output sequence. The attention mechanism can be represented mathematically as follows:\\nattention\\nThe feed-forward network is responsible for transforming the target hidden state into a representation that is compatible with the attention mechanism. This architecture allows the model to focus on different parts of the input sequence during the translation process, improving the alignment and quality of the translations.'}]\n",
            "\n",
            "Agent Response: ### LoRA in Machine Learning:\n",
            "- **LoRA** stands for **Low-Rank Adaptation**. It is a lightweight training technique used for fine-tuning Large Language and Stable Diffusion Models without needing full model training. LoRA reduces the computational burden, allowing for faster adaptation of models. It is particularly useful for fine-tuning large language models efficiently. You can learn more about LoRA from this [source](https://machinelearningmastery.com/using-lora-in-stable-diffusion/).\n",
            "\n",
            "### Tim Dettmers:\n",
            "- **Tim Dettmers** is a researcher and educator in deep learning and machine learning. He has contributed to various research papers and conferences in the field of artificial intelligence. Some of his work includes Convolutional 2D Knowledge Graph Embeddings and 8-Bit Approximations for Parallelism in Deep Learning. You can find more about Tim Dettmers on his [website](https://timdettmers.com/).\n",
            "\n",
            "### Attention in Machine Learning:\n",
            "- **Attention** is a mechanism in machine learning that allows models to focus on specific parts of the input sequence when making predictions. It helps the model dynamically highlight and use the salient parts of the information at hand, similar to how the human brain uses attention. Attention mechanisms are commonly used in tasks like natural language processing and image recognition to improve model performance. You can read more about Attention in machine learning from this [source](https://machinelearningmastery.com/what-is-attention/).\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
